{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c18451",
   "metadata": {},
   "source": [
    "<b>Ensemble learning</b> combines several base algorithms to form one optimized predictive algorithm. Ensemble learning methods are employed for several reasons, each addressing specific challenges in machine learning:<br>\n",
    "1- <b>Improved Accuracy</b> (By combining the predictions of multiple models, ensemble methods can often achieve better performance than any individual model.)<br>\n",
    "2- <b>Robustness to Overfitting</b><br>\n",
    "3- <b>Handling Noisy Data</b><br>\n",
    "4- <b>Model Flexibility </b> (Ensemble methods can combine different types of models or variations of the same model to leverage their respective strengths. This flexibility allows ensemble methods to adapt to diverse types of data and modeling challenges.)<br>\n",
    "\n",
    "<b>Types Of Ensemble Methods</b><br>\n",
    "Ensemble Methods can be used for various reasons, mainly to:<br>\n",
    "Decrease Variance (Bagging)<br>\n",
    "Decrease Bias (Boosting)<br>\n",
    "Improve Predictions (Stacking)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd032f",
   "metadata": {},
   "source": [
    "### Popular Boosting Algorithms :-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e0c541",
   "metadata": {},
   "source": [
    "####  Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0041fbf",
   "metadata": {},
   "source": [
    "AdaBoost, short for Adaptive Boosting, is a popular ensemble learning technique used in machine learning. It's designed to improve the performance of weak learners (typically decision trees) by combining them into a strong learner<br>\n",
    "Read about it in this article:- https://blog.paperspace.com/adaboost-optimizer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a08537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\envs\\dataml100\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45dee972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.1 3.5 1.4 0.2]\n",
      "0 (150,)\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print(X[0])\n",
    "print(y[0],y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9eb5c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "807711d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaBoostClassifier takes Decision Tree as its learner model\n",
    "#AdaBoost can be used with SVM classifiers \n",
    "#AdaBoost can be used with KNN classifiers\n",
    "#AdaBoost can be used with Naive Bayes classifiers\n",
    "#For Regression Tasks AdaBoostRegressorcan be used.\n",
    "model= AdaBoostClassifier(n_estimators=50,learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aed0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f27bcaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b64ae229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7370a3a4",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add2770",
   "metadata": {},
   "source": [
    "AdaBoost traditionally uses decision trees stumps as base estimators and has limited flexibility in this regard,<br>\n",
    "When the target column is continuous, we use Gradient Boosting Regressor whereas when it is a classification problem, we use Gradient Boosting Classifier. The only difference between the two is the “Loss function”. The objective here is to minimize this loss function by adding weak learners using gradient descent. Since it is based on loss function hence for regression problems, we’ll have different loss functions like Mean squared error (MSE) and for classification, we will have different for e.g log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a554a81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
